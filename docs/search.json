[
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "This page contains an outline of the topics, content, and assignments for the semester. Note that this schedule will be updated as the semester progresses and the timeline of topics and assignments might be updated throughout the semester.\n\n\n\n\n\n\n\n\nWEEK\nDATE\nTOPIC\nPREPARE\nMATERIALS\nDUE\n\n\n\n\n1\nMon, Aug 28\nWelcome to STA 101!\n\n\nğŸ–¥ï¸ slides  â˜ï¸ ae-01  âŒ¨ï¸ ae-01\n\n\n\n\n\n\nWed, Aug 30\nHello data\nğŸ“– ims: Chp 1  ğŸ’» tutorial: 01-data-01 |&gt; submit\nğŸ–¥ï¸ slides  â˜ï¸ ae-02  âŒ¨ï¸ ae-02  ğŸ—ï¸ ae-02-sa\nğŸ”˜ Survey: Getting to know you\n\n\n\n\nFri, Sep 1\nLab 1: Hello R!\n\n\nâŒ¨ï¸ lab-1\n\n\n\n\n2\nMon, Sep 4\nâš’ï¸ Labor Day - No lecture\n\n\n\n\n\n\n\n\n\n\nWed, Sep 6\nStudy design\nğŸ“– ims: Chp 2  ğŸ’» tutorial: 01-data-02 |&gt; submit  ğŸ’» tutorial: 01-data-03 |&gt; submit\nğŸ–¥ï¸ slides  â˜ï¸ ae-03  âŒ¨ï¸ ae-03  ğŸ—ï¸ ae-03-sa\n\n\n\n\n\n\nThu, Sep 7\n\n\n\n\n\n\nâœ… Lab 1 at 5 pm\n\n\n\n\nFri, Sep 8\nLab 2: Hello data!\nğŸ“– ims: Chp 3  ğŸ’» tutorial: 01-data-04 |&gt; submit\nâŒ¨ï¸ lab-2\n\n\n\n\n3\nMon, Sep 11\nExploring categorical data\nğŸ“– ims: Chp 4  ğŸ’» tutorial: 02-explore-01 |&gt; submit\nğŸ–¥ï¸ slides  â˜ï¸ ae-04  âŒ¨ï¸ ae-04  ğŸ—ï¸ ae-04-sa\n\n\n\n\n\n\nWed, Sep 13\nExploring numerical data\nğŸ“– ims: Chp 5  ğŸ’» tutorial: 02-explore-02 |&gt; submit\nğŸ–¥ï¸ slides  â˜ï¸ ae-05  âŒ¨ï¸ ae-05  ğŸ—ï¸ ae-05-sa\n\n\n\n\n\n\nThu, Sep 14\n\n\n\n\n\n\nâœ… Lab 2 at 5 pm\n\n\n\n\nFri, Sep 15\nLab 3: Exploratory data analysis\nğŸ“– ims: Chp 6  ğŸ’» tutorial: 02-explore-03 |&gt; submit\nâŒ¨ï¸ lab-3\n\n\n\n\n4\nMon, Sep 18\nRegression with a single predictor I  Guest Lecture: Dr.Â Alex Fisher\nğŸ“– ims: Chp 7.1 - 7.2  ğŸ’» tutorial: 03-model-01 |&gt; submit  ğŸ’» tutorial: 03-model-02 |&gt; submit \nğŸ–¥ï¸ slides  â˜ï¸ ae-06  âŒ¨ï¸ ae-06  ğŸ—ï¸ ae-06-sa\n\n\n\n\n\n\nWed, Sep 20\nNo class  Catch up with readings and interactive tutorials\nğŸ“– ims: Chp 7.2 - 7.4  ğŸ’» tutorial: 03-model-03 |&gt; submit\n\n\n\n\n\n\n\n\nThu, Sep 21\n\n\n\n\n\n\nâœ… Lab 3 at 5 pm\n\n\n\n\nFri, Sep 22\nLab 4: Regression\n\n\nâŒ¨ï¸ lab-4\n\n\n\n\n5\nMon, Sep 25\nRegression with multiple predictors\nğŸ“– ims: Chp 8.1 - 8.3\nğŸ–¥ï¸ slides  â˜ï¸ ae-07  âŒ¨ï¸ ae-07  ğŸ—ï¸ ae-07-sa\n\n\n\n\n\n\nWed, Sep 27\nModel selection\nğŸ“– ims: Chp 8.4 - 8.5\nğŸ–¥ï¸ slides  â˜ï¸ ae-08  âŒ¨ï¸ ae-08  ğŸ—ï¸ ae-08-sa\n\n\n\n\n\n\nThu, Sep 28\n\n\n\n\n\n\nâœ… Lab 4 at 5 pm\n\n\n\n\nFri, Sep 29\nExam 1 Review\n\n\nğŸ“ er-1  ğŸ—ï¸ er-1-sa\n\n\n\n\n6\nMon, Oct 2\nRegression overview\n\n\nğŸ–¥ï¸ slides\n\n\n\n\n\n\nWed, Oct 4\nExam 1 - In class\n\n\n\n\n\n\n\n\n\n\nFri, Oct 6\nNo lab: Work on Exam 1 - Take home\n\n\n\n\nExam 1 - Take home + Tutorials at 5 pm\n\n\n7\nMon, Oct 9\nLogistic regression I\nğŸ“– ims: Chp 9.1 - 9.2\nğŸ–¥ï¸ slides  â˜ï¸ ae-09  âŒ¨ï¸ ae-09  ğŸ—ï¸ ae-09-sa\n\n\n\n\n\n\nWed, Oct 11\nLogistic regression II\nğŸ“– ims: Chp 9.3 - 9.5\nğŸ–¥ï¸ slides\n\n\n\n\n\n\nFri, Oct 13\nProject 1 workday\n\n\n\n\nğŸ“„ Project 1 at 5 pm\n\n\n8\nMon, Oct 16\nğŸ‚ Fall Break - No lecture\n\n\n\n\n\n\n\n\n\n\nWed, Oct 18\nHypothesis testing with randomization\nğŸ“– ims: Chp 11  ğŸ’» tutorial: 04-foundations-01 |&gt; submit  ğŸ’» tutorial: 04-foundations-02 |&gt; submit\nğŸ–¥ï¸ slides  â˜ï¸ ae-10  âŒ¨ï¸ ae-10  ğŸ—ï¸ ae-10\n\n\n\n\n\n\nFri, Oct 20\nLab 5: Hypothesis testing\n\n\nâŒ¨ï¸ lab-5\n\n\n\n\n9\nMon, Oct 23\nConfidence intervals with bootstrapping\nğŸ“– ims: Chp 12  ğŸ’» tutorial: 04-foundations-04 |&gt; submit\nğŸ–¥ï¸ slides\n\n\n\n\n\n\nWed, Oct 25\nDecision errors  Guest Lecture: Dr.Â Yue Jiang\nğŸ“– ims: Chp 14  ğŸ’» tutorial: 04-foundations-03 |&gt; submit\nğŸ–¥ï¸ slides\n\n\n\n\n\n\nThu, Oct 26\n\n\n\n\n\n\nâœ… Lab 5 at 5 pm\n\n\n\n\nFri, Oct 27\nLab 6: Foundations of inference\n\n\nâŒ¨ï¸ lab-6\n\n\n\n\n10\nMon, Oct 30\nInference with mathematical models\nğŸ“– ims: Chp 13\nğŸ–¥ï¸ slides  â˜ï¸ ae-11  âŒ¨ï¸ ae-11  ğŸ—ï¸ ae-11\n\n\n\n\n\n\nWed, Nov 1\nZoom lecture:  Inference for a single proportion\nğŸ“– ims: Chp 16  ğŸ’» tutorial: 05-infer-01 |&gt; submit\nğŸ–¥ï¸ slides  â˜ï¸ ae-12  âŒ¨ï¸ ae-12  ğŸ—ï¸ ae-12\n\n\n\n\n\n\nThu, Nov 2\n\n\n\n\n\n\nâœ… Lab 6 at 5 pm\n\n\n\n\nFri, Nov 3\nLab 7: Inference for proportions\n\n\nâŒ¨ï¸ lab-7\n\n\n\n\n11\nMon, Nov 6\nInference for comparing two proportions\nğŸ“– ims: Chp 17  ğŸ’» tutorial: 05-infer-02 |&gt; submit\nğŸ–¥ï¸ slides  â˜ï¸ ae-13  âŒ¨ï¸ ae-13  ğŸ—ï¸ ae-13\n\n\n\n\n\n\nWed, Nov 8\nInference for two-way tables\nğŸ“– ims: Chp 18  ğŸ’» tutorial: 05-infer-03 |&gt; submit\nğŸ–¥ï¸ slides  â˜ï¸ ae-14  âŒ¨ï¸ ae-14  ğŸ—ï¸ ae-14\n\n\n\n\n\n\nThu, Nov 9\n\n\n\n\n\n\nâœ… Lab 7 at 5 pm\n\n\n\n\nFri, Nov 10\nExam 2 Review\nğŸ’» tutorial: 05-infer-04 |&gt; submit\nğŸ“ er-2  ğŸ—ï¸ er-2-sa\n\n\n\n\n12\nMon, Nov 13\nOverview: Inference for proportions\nğŸ“– ims: Chp 23.1\nğŸ–¥ï¸ slides\n\n\n\n\n\n\nWed, Nov 15\nExam 2 - In class\n\n\n\n\n\n\n\n\n\n\nFri, Nov 17\nNo lab: Work on Exam 2 - Take home\n\n\n\n\nExam 2 - Take home + Tutorials at 5 pm\n\n\n13\nMon, Nov 20\nInference for a single mean\nğŸ“– ims: Chp 19  ğŸ’» tutorial: 05-infer-05 |&gt; submit  ğŸ’» tutorial: 05-infer-06 |&gt; submit\nğŸ–¥ï¸ slides  â˜ï¸ ae-15  âŒ¨ï¸ ae-15  ğŸ—ï¸ ae-15\n\n\n\n\n\n\nWed, Nov 22\nğŸ¦ƒ Thanksgiving Break - No lecture\n\n\n\n\n\n\n\n\n\n\nFri, Nov 24\nğŸ¦ƒ Thanksgiving Break - No lab\n\n\n\n\n\n\n\n\n14\nMon, Nov 27\nInference for comparing two independent means\nğŸ“– ims: Chp 20  ğŸ’» tutorial: 05-infer-07 |&gt; submit\nğŸ–¥ï¸ slides  â˜ï¸ ae-16  âŒ¨ï¸ ae-16\n\n\n\n\n\n\nWed, Nov 29\nInference for comparing two paired + more means\nğŸ“– ims: Chp 21\nğŸ–¥ï¸ slides  â˜ï¸ ae-17\n\n\n\n\n\n\nFri, Dec 1\nProject proposal conversation  Lab 8: Inference for means\n\n\nâŒ¨ï¸ lab-8 [Optional]\nğŸ“„ Project 2 proposal conversation\n\n\n15\nMon, Dec 4\nEthics\nğŸ¥ Data privacy  ğŸ¥ Misrepresentation  ğŸ¥ Algorithmic bias  ğŸ¥ Joy Buolamwini - How Iâ€™m fighting bias in algorithms  ğŸ¥ Cathy Oâ€™Neil - Weapons of Math Destruction  ğŸ¥ Safiya Umoja Noble - Imagining a Future Free from the Algorithms of Oppression\nğŸ–¥ï¸ slides\n\n\n\n\n\n\nWed, Dec 6\nTelling a (data) story\nğŸ“– fdv: Chp 29\nğŸ–¥ï¸ slides\n\n\n\n\n\n\nThu, Dec 7\n\n\n\n\n\n\nâœ… Lab 8 at 5 pm\n\n\n\n\nFri, Dec 8\nProject 2 peer review\n\n\n\n\nğŸ“„ Project 2 peer review\n\n\n16\nThu, Dec 14\n\n\n\n\n\n\nğŸ“„ Project 2 at 2 pm"
  },
  {
    "objectID": "lectures/lecture-6.html",
    "href": "lectures/lecture-6.html",
    "title": "Advanced imports and data manipulations",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "ğŸ“œ Slides",
      "06 - Advanced Data Manipulation"
    ]
  },
  {
    "objectID": "lectures/lecture-6.html#slides",
    "href": "lectures/lecture-6.html#slides",
    "title": "Advanced imports and data manipulations",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "ğŸ“œ Slides",
      "06 - Advanced Data Manipulation"
    ]
  },
  {
    "objectID": "lectures/lecture-4.html",
    "href": "lectures/lecture-4.html",
    "title": "Data visualization - ggplot2 and beyond",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "ğŸ“œ Slides",
      "04 - Data Visualization with ggplot2"
    ]
  },
  {
    "objectID": "lectures/lecture-4.html#slides",
    "href": "lectures/lecture-4.html#slides",
    "title": "Data visualization - ggplot2 and beyond",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "ğŸ“œ Slides",
      "04 - Data Visualization with ggplot2"
    ]
  },
  {
    "objectID": "lectures/lecture-2.html",
    "href": "lectures/lecture-2.html",
    "title": "Data cleaning & wrangling - Tidyverse",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "ğŸ“œ Slides",
      "02 - Data Wrangling with Tidyverse"
    ]
  },
  {
    "objectID": "lectures/lecture-2.html#slides",
    "href": "lectures/lecture-2.html#slides",
    "title": "Data cleaning & wrangling - Tidyverse",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "ğŸ“œ Slides",
      "02 - Data Wrangling with Tidyverse"
    ]
  },
  {
    "objectID": "lectures/lecture-0.html",
    "href": "lectures/lecture-0.html",
    "title": "Hello there!",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "ğŸ“œ Slides",
      "00 - Hello there!"
    ]
  },
  {
    "objectID": "lectures/lecture-0.html#slides",
    "href": "lectures/lecture-0.html#slides",
    "title": "Hello there!",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "ğŸ“œ Slides",
      "00 - Hello there!"
    ]
  },
  {
    "objectID": "final-project.html",
    "href": "final-project.html",
    "title": "Final Project: Smart City Data Hackathon",
    "section": "",
    "text": "In this final project, you will work in teams to explore, analyze, and visualize urban data to propose data-driven insights or solutions that could improve the functioning of New York City.\nYou will combine everything learned in the course â€” from data import and cleaning to visualization, spatial analysis, and reproducible reporting.",
    "crumbs": [
      "ğŸ› ï¸Final Project"
    ]
  },
  {
    "objectID": "final-project.html#learning-objectives",
    "href": "final-project.html#learning-objectives",
    "title": "Final Project: Smart City Data Hackathon",
    "section": "",
    "text": "In this final project, you will work in teams to explore, analyze, and visualize urban data to propose data-driven insights or solutions that could improve the functioning of New York City.\nYou will combine everything learned in the course â€” from data import and cleaning to visualization, spatial analysis, and reproducible reporting.",
    "crumbs": [
      "ğŸ› ï¸Final Project"
    ]
  },
  {
    "objectID": "final-project.html#the-challenge",
    "href": "final-project.html#the-challenge",
    "title": "Final Project: Smart City Data Hackathon",
    "section": "ğŸ§© The challenge",
    "text": "ğŸ§© The challenge\nThe City of New York wants to better understand patterns in mobility, safety, and urban environment. Your task is to use open datasets (such as NYC Taxi data, 311 complaints, weather, or spatial information) to:\n\nIdentify a problem or opportunity for the city, analyze it with data, and propose one or more practical, data-informed solutions.\n\nYour project should tell a clear story supported by data, code, and visualization.",
    "crumbs": [
      "ğŸ› ï¸Final Project"
    ]
  },
  {
    "objectID": "final-project.html#team-format",
    "href": "final-project.html#team-format",
    "title": "Final Project: Smart City Data Hackathon",
    "section": "ğŸ§‘â€ğŸ¤â€ğŸ§‘ Team format",
    "text": "ğŸ§‘â€ğŸ¤â€ğŸ§‘ Team format\n\nWork in teams of 2â€“3 students.\nEach team chooses its own research question and data sources.\nCollaboration and creativity are key â€” technical perfection is less important than originality, storytelling, and insight.",
    "crumbs": [
      "ğŸ› ï¸Final Project"
    ]
  },
  {
    "objectID": "final-project.html#suggested-data-sources",
    "href": "final-project.html#suggested-data-sources",
    "title": "Final Project: Smart City Data Hackathon",
    "section": "ğŸ“¦ Suggested data sources",
    "text": "ğŸ“¦ Suggested data sources\nYou are encouraged to combine several datasets. Here are examples (but you may choose others):\nCore dataset\n\nNYC Taxi & Limousine Commission Trip Data https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\n\nAdditional layers (choose 1â€“2+)\n\nWeather data â€” NOAA, Open-Meteo API, or other public APIs\n311 Service Requests â€” NYC Open Data Portal\nTraffic volume, accidents, or speed data\nPublic transportation, bike sharing (CitiBike), or parking data\nNeighborhood demographics or land use\nSpatial data â€” borough boundaries, taxi zones, street networks",
    "crumbs": [
      "ğŸ› ï¸Final Project"
    ]
  },
  {
    "objectID": "final-project.html#what-your-project-should-include",
    "href": "final-project.html#what-your-project-should-include",
    "title": "Final Project: Smart City Data Hackathon",
    "section": "ğŸ—ºï¸ What your project should include",
    "text": "ğŸ—ºï¸ What your project should include\n\nData import and integration\n\nCombine at least two independent datasets (e.g.Â taxi + weather, or taxi + 311 complaints).\nUse advanced import tools (arrow, duckdb, polars, or API access).\n\nData transformation and exploration\n\nClean and summarize data.\nUse dplyr, sf, or polars for efficient manipulation.\n\nSpatial or temporal visualization\n\nCreate at least one map or time series plot.\nUse ggplot2, sf, or leaflet.\n\nInsights and recommendations\n\nDescribe findings clearly.\nPropose 1â€“2 potential solutions, policies, or improvements for the city.\n\nReproducibility and storytelling\n\nDocument your process in a Quarto report.\nInclude code, figures, and narrative in a cohesive story.",
    "crumbs": [
      "ğŸ› ï¸Final Project"
    ]
  },
  {
    "objectID": "final-project.html#examples-of-possible-directions",
    "href": "final-project.html#examples-of-possible-directions",
    "title": "Final Project: Smart City Data Hackathon",
    "section": "ğŸŒŸ Examples of possible directions",
    "text": "ğŸŒŸ Examples of possible directions\n\nğŸ•’ Mobility efficiency: â€œHow do traffic jams and weather affect trip duration?â€\nğŸ’° Economic behavior: â€œWhich neighborhoods have the highest average tips and why?â€\nğŸ§­ Accessibility: â€œWhere are the underserved zones with poor transport connectivity?â€\nğŸŒ³ Sustainability: â€œHow can the city optimize taxi demand to reduce COâ‚‚ emissions?â€\nğŸ§¹ Urban services: â€œAre 311 complaints correlated with low taxi activity or certain areas?â€\nğŸ—ºï¸ Safety: â€œMapping taxi accident locations and suggesting safer routes.â€",
    "crumbs": [
      "ğŸ› ï¸Final Project"
    ]
  },
  {
    "objectID": "final-project.html#submission",
    "href": "final-project.html#submission",
    "title": "Final Project: Smart City Data Hackathon",
    "section": "ğŸ“¤ Submission",
    "text": "ğŸ“¤ Submission\nEach team must submit:\n\nA Quarto project (one .qmd or multi-file report) including:\n\nIntroduction: research question and motivation.\nData import and cleaning process.\nAnalysis, visualization, and interpretation.\nPolicy or design recommendations.\n\nA short presentation (5â€“7 minutes) during the hackathon session:\n\nTell your story visually.\nFocus on key insights and proposed solutions.\n\nLink to your published Quarto report (on Quarto Pub or GitHub Pages).",
    "crumbs": [
      "ğŸ› ï¸Final Project"
    ]
  },
  {
    "objectID": "final-project.html#final-note",
    "href": "final-project.html#final-note",
    "title": "Final Project: Smart City Data Hackathon",
    "section": "ğŸ§­ Final note",
    "text": "ğŸ§­ Final note\nThis project is your opportunity to think like data scientists working for a city â€” where code meets impact. Be bold, creative, and analytical. Surprise us with something the data hides!",
    "crumbs": [
      "ğŸ› ï¸Final Project"
    ]
  },
  {
    "objectID": "course-syllabus.html",
    "href": "course-syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Welcome, young explorers!\nThis semester, we are setting off on a thrilling scientific expedition to the icy shores of Antarctica. Our lead guide is an experienced polar explorer, a penguin named Pipe1. He knows everything about data, because his whole life is data: the length of his beak, his wingspan, and even his favorite type of fish!\n1Â He got his name for his amazing ability to combine different actions into one smooth and logical flow, as if passing data through an invisible â€œpipelineâ€ (|&gt;). This is the approach he wants to pass on to you!Our mission is to learn the R language to understand penguins, analyze their lives, and present our findings to the world. Forget about boring lectures. We have campfire briefings, field research, and a final Grand Penguin Gala Presentation awaiting us!\n\n\n\n\n\nOnce upon a time, in faraway Antarctica, penguins lived their quiet lives until the first explorers arrived and started collecting data on their behavior. Over time, this data turned into massive sets of information that needed analysis. That was when Pipe appeared, a researcher penguin who learned to use R to understand his world. Now Pipe wants to share his knowledge with you so that you can help him uncover the secrets of penguin life.\n\n\n\nLately, Pipe has noticed something strange. The population of his favorite fish, the ice sardine, has started to decline sharply near the coast. What is happening? Is it climate change? A new predator? Or perhaps the fish have simply migrated?\n\n\n\nOur mission is to use the full power of R to analyze data on water temperature, ocean currents, the activity of other animals, and find the answer. The final project will serve as your report to the International Penguin Council, where you will present your findings and save the future of the ice sardines!\n\n\n\n\n\n\n\nğŸ§  An open mind and a thirst for knowledge.\nğŸ¤“ A desire to experiment and no fear of making mistakes.\nğŸ¤ Readiness to work in a team and share ideas.\nğŸ§¥ Comfortable clothing for cold weather (metaphoricallyâ€”a comfortable working environment on your computer).\n\n\n\n\n\nMaster the basics of programming in R and using RStudio.\nLearn to collect, clean, and analyze data.\nDevelop data visualization skills using ggplot2.\nPrepare and present your own research in the form of a Quarto report.\n\n\n\n\n\n\n\n\nYou will be able to confidently write code in R for data analysis.\nYou will learn to create informative visualizations.\nYou will gain experience working with real datasets.\nYou will be able to present your findings in the form of a professional report.\n\n\n\n\nOur route includes several exciting stages:\n\n\n\n\nğŸ”¥Campfire Briefings (Lectures): Here, our lead guide (the instructor) and Pipe will share wisdom: showing maps, teaching how to use tools (R and RStudio), and telling stories about great discoveries in the world of data.\nğŸ§ªField Research (Practical Labs): Time to get your hands dirty! We will go out onto the â€œice floesâ€ (real datasets), learn to clean snow off the data, build charts that shine like the aurora borealis, and make our own discoveries.\nğŸ“ˆWeekly Expedition Reports (Homework): Every explorer keeps a logbook. You will document your findings, write code, and share results in the format of Quarto reports.\nğŸŠGrand Penguin Gala Presentation (Final Project): The culmination of our journey! You (solo or in a team) will present your own researchâ€”from data collection to final conclusionsâ€”before the esteemed council of penguins (and the instructor).",
    "crumbs": [
      "ğŸ‘‹Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#history-of-the-expedition",
    "href": "course-syllabus.html#history-of-the-expedition",
    "title": "Syllabus",
    "section": "",
    "text": "Once upon a time, in faraway Antarctica, penguins lived their quiet lives until the first explorers arrived and started collecting data on their behavior. Over time, this data turned into massive sets of information that needed analysis. That was when Pipe appeared, a researcher penguin who learned to use R to understand his world. Now Pipe wants to share his knowledge with you so that you can help him uncover the secrets of penguin life.",
    "crumbs": [
      "ğŸ‘‹Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#the-mystery-of-the-expedition",
    "href": "course-syllabus.html#the-mystery-of-the-expedition",
    "title": "Syllabus",
    "section": "",
    "text": "Lately, Pipe has noticed something strange. The population of his favorite fish, the ice sardine, has started to decline sharply near the coast. What is happening? Is it climate change? A new predator? Or perhaps the fish have simply migrated?\n\n\n\nOur mission is to use the full power of R to analyze data on water temperature, ocean currents, the activity of other animals, and find the answer. The final project will serve as your report to the International Penguin Council, where you will present your findings and save the future of the ice sardines!",
    "crumbs": [
      "ğŸ‘‹Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#what-to-pack-for-the-expedition",
    "href": "course-syllabus.html#what-to-pack-for-the-expedition",
    "title": "Syllabus",
    "section": "",
    "text": "ğŸ§  An open mind and a thirst for knowledge.\nğŸ¤“ A desire to experiment and no fear of making mistakes.\nğŸ¤ Readiness to work in a team and share ideas.\nğŸ§¥ Comfortable clothing for cold weather (metaphoricallyâ€”a comfortable working environment on your computer).",
    "crumbs": [
      "ğŸ‘‹Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#expedition-goals",
    "href": "course-syllabus.html#expedition-goals",
    "title": "Syllabus",
    "section": "",
    "text": "Master the basics of programming in R and using RStudio.\nLearn to collect, clean, and analyze data.\nDevelop data visualization skills using ggplot2.\nPrepare and present your own research in the form of a Quarto report.",
    "crumbs": [
      "ğŸ‘‹Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#expedition-results",
    "href": "course-syllabus.html#expedition-results",
    "title": "Syllabus",
    "section": "",
    "text": "You will be able to confidently write code in R for data analysis.\nYou will learn to create informative visualizations.\nYou will gain experience working with real datasets.\nYou will be able to present your findings in the form of a professional report.",
    "crumbs": [
      "ğŸ‘‹Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#what-awaits-us",
    "href": "course-syllabus.html#what-awaits-us",
    "title": "Syllabus",
    "section": "",
    "text": "Our route includes several exciting stages:\n\n\n\n\nğŸ”¥Campfire Briefings (Lectures): Here, our lead guide (the instructor) and Pipe will share wisdom: showing maps, teaching how to use tools (R and RStudio), and telling stories about great discoveries in the world of data.\nğŸ§ªField Research (Practical Labs): Time to get your hands dirty! We will go out onto the â€œice floesâ€ (real datasets), learn to clean snow off the data, build charts that shine like the aurora borealis, and make our own discoveries.\nğŸ“ˆWeekly Expedition Reports (Homework): Every explorer keeps a logbook. You will document your findings, write code, and share results in the format of Quarto reports.\nğŸŠGrand Penguin Gala Presentation (Final Project): The culmination of our journey! You (solo or in a team) will present your own researchâ€”from data collection to final conclusionsâ€”before the esteemed council of penguins (and the instructor).",
    "crumbs": [
      "ğŸ‘‹Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#interests",
    "href": "course-syllabus.html#interests",
    "title": "Syllabus",
    "section": "ğŸ’¡Interests",
    "text": "ğŸ’¡Interests\n\nğŸ­ Penguin behavior analysis\nğŸ–¼ï¸ Data visualization\nğŸ¤– Machine learning\nğŸŒ Antarctic ecology",
    "crumbs": [
      "ğŸ‘‹Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#philosophy",
    "href": "course-syllabus.html#philosophy",
    "title": "Syllabus",
    "section": "ğŸ’­Philosophy",
    "text": "ğŸ’­Philosophy\n\nâ€œData isnâ€™t just numbers. These are stories waiting to be told.â€",
    "crumbs": [
      "ğŸ‘‹Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#likes",
    "href": "course-syllabus.html#likes",
    "title": "Syllabus",
    "section": "ğŸ‘Likes",
    "text": "ğŸ‘Likes\n\nğŸŸ Favorite food: Fresh fish\nğŸ¶ Favorite music: Sounds of the ocean and calls of other penguins\nâœ¨ Hobbies: Swimming, exploring new territories, and playing snowballs",
    "crumbs": [
      "ğŸ‘‹Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#fun-facts",
    "href": "course-syllabus.html#fun-facts",
    "title": "Syllabus",
    "section": "âœ…Fun Facts",
    "text": "âœ…Fun Facts\n\nğŸ› ï¸ Pipe learned to code by watching researchers.\nğŸ—ƒï¸ He created his own dataset on penguin behavior, which is used in scientific studies.\nğŸ« Pipe dreams of one day opening a school for penguins where they can learn data science.",
    "crumbs": [
      "ğŸ‘‹Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#expedition-logbook-route-details",
    "href": "course-syllabus.html#expedition-logbook-route-details",
    "title": "Syllabus",
    "section": "ğŸ“–Expedition Logbook: Route Details",
    "text": "ğŸ“–Expedition Logbook: Route Details\n\n\n\n\nWeek 1: ğŸ§³ Packing Suitcases\n\nTopic: R Basics, IDE, Data Types.\n\nTask: Researchers from the â€˜Palmerâ€™ Antarctic station have sent us the first file. It contains basic data on the penguin population. Our first task is to open this â€œevidence containerâ€ in RStudio, check if all data is in place, and determine where we have numbers (beak length), where we have text (penguin species), and where we have logical variables (whether a chick was sighted). This is the foundation of our investigation, and we must be confident in the quality of our first evidence.\nResult of Report #1: The researcher loads the data, uses functions str(), summary(), class(), and writes a short conclusion.\n\n\n\n\n\nWeek 2: ğŸ§ First Steps on Ice\n\nTopic: dplyr, tidyr (data transformation).\n\nTask: We received a new file about the temperature regime. But the data was collected from several buoys, and it is, to put it mildly, messy. Somewhere there are gaps (NA), somewhere the temperature is in Fahrenheit instead of Celsius. Your task is to â€œclean up the crime sceneâ€: using dplyr, filter out records from broken buoys, using mutate(), create a new column with temperature in Celsius, and using select(), keep only the most important columns. We cannot look for anomalies in dirty data.\nResult of Report #2: The researcher provides code for cleaning the data and the final â€œcleanâ€ table.\n\n\n\n\n\n\n\n\nWeek 3: ğŸ““ Keeping the Captainâ€™s Log\n\nTopic: Quarto (reports).\n\nTask: The International Penguin Council demands the first interim report. They donâ€™t want to see just code. They need a professional document. Your task is to format the results of the previous week (cleaned data) into a Quarto report. Add headings, a text description of your actions, the code itself, and the output (e.g., the first 6 rows of the clean table). This is our first official document in the case of the disappearing sardines!\nResult of Report #3: A professionally formatted PDF or HTML report documenting the first steps of the investigation.\n\n\n\n\n\nWeek 4: ğŸ—ºï¸ Visualizing Colony Data\n\nTopic: ggplot2 (visualization).\n\nTask: Dry numbers are good, but we need a picture! Using cleaned temperature data and sardine population data, create a chart using ggplot2. On the X-axisâ€”years, on the Y-axisâ€”water temperature and sardine count. Is there a visual correlation? Is it true that when the water gets warmer, there are fewer fish? This could be our first serious piece of evidence!\nResult of Report #4: A chart with a conclusion: Visual analysis shows a potential inverse relationship between water temperature and sardine population. Further research is needed.\n\n\n\n\n\n\n\n\nWeek 5: ğŸ“¡ Listening to Penguin Talk\n\nTopic: Web scraping, API.\n\nTask: One hypothesis is the change in sea ice area. We donâ€™t have this data, but the National Snow and Ice Data Center (NSIDC) publishes it online. Your task is to collect data on Antarctic ice area for the last 10 years using rvest (or via API, if available). Is there a declining trend? This could be another factor affecting the ecosystem.\nResult of Report #5: A script for data collection and the obtained dataset with a short description.\n\n\n\n\n\nWeek 6: ğŸ” Techniques for Advanced Explorers\n\nTopic: DuckDB, Arrow (big data).\n\nTask: We were handed a giant archive. It contains millions of records on fishing vessel activity. Opening it in Excel is suicide. Use Arrow and DuckDB to efficiently analyze this data without loading it into memory. Our question: has fishing activity increased in the sardine disappearance area in recent years?\nResult of Report #6: An SQL query via DuckDB to the file and the result (e.g., an aggregated table by years) with a conclusion.\n\n\n\n\n\n\n\n\nWeek 7: ğŸ—º Mapping Uncharted Lands\n\nTopic: sf (spatial analysis).\n\nTask: We have coordinates of temperature buoys, fishing data, and places where sardines disappeared. Time to put it all on a map! Using sf, create a map of the region. Mark thermal anomalies (highest temperature), intense fishing zones, and places where sardines were seen previously. are there â€œhot spotsâ€ where all these factors intersect? This could be the epicenter of our mystery.\nResult of Report #7: A map of the region with data layers applied and a conclusion about the spatial overlap of factors.\n\n\n\n\n\nWeek 8: ğŸ‰ Grand Penguin Gala Presentation!\n\nTopic: Final Project.\n\nTask: Zero Hour. You have collected all the evidence. Now combine the results of all previous reports (text, code, charts, maps) into one final, comprehensive report in Quarto. Draw a clear conclusion, answering the main question: what happened to the ice sardine? Present your theory, backed by the data you analyzed throughout the expedition.",
    "crumbs": [
      "ğŸ‘‹Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "computing-cheatsheets.html",
    "href": "computing-cheatsheets.html",
    "title": "R cheatsheets",
    "section": "",
    "text": "The following cheatsheets come from https://posit.co/resources/cheatsheets. We havenâ€™t covered every function and functionality listed on them, but you might still find them useful as references.",
    "crumbs": [
      "âœï¸Cheatsheets"
    ]
  },
  {
    "objectID": "assignments/lab-6.html",
    "href": "assignments/lab-6.html",
    "title": "Assignment 06: Advanced Imports and Data Manipulations",
    "section": "",
    "text": "After completing this assignment, you will be able to:\n\nWork with large, partitioned datasets (DuckDB, Arrow, or Polars).\nExplore and visualize time-dependent or categorical patterns.\nReproducibly document a complete analysis pipeline.",
    "crumbs": [
      "ğŸ‹ï¸ Weekly assignments",
      "06 - Advanced Imports and Data Manipulations"
    ]
  },
  {
    "objectID": "assignments/lab-6.html#learning-objectives",
    "href": "assignments/lab-6.html#learning-objectives",
    "title": "Assignment 06: Advanced Imports and Data Manipulations",
    "section": "",
    "text": "After completing this assignment, you will be able to:\n\nWork with large, partitioned datasets (DuckDB, Arrow, or Polars).\nExplore and visualize time-dependent or categorical patterns.\nReproducibly document a complete analysis pipeline.",
    "crumbs": [
      "ğŸ‹ï¸ Weekly assignments",
      "06 - Advanced Imports and Data Manipulations"
    ]
  },
  {
    "objectID": "assignments/lab-6.html#dataset",
    "href": "assignments/lab-6.html#dataset",
    "title": "Assignment 06: Advanced Imports and Data Manipulations",
    "section": "ğŸ“¦ Dataset",
    "text": "ğŸ“¦ Dataset\nIn this assignment, you will work with New York City Taxi & Limousine Commission (TLC) Trip Record Data: ğŸ‘‰ https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\nThese open datasets contain detailed trip records from NYC taxis and for-hire vehicles (FHV, green/yellow cabs, etc.).",
    "crumbs": [
      "ğŸ‹ï¸ Weekly assignments",
      "06 - Advanced Imports and Data Manipulations"
    ]
  },
  {
    "objectID": "assignments/lab-6.html#your-task",
    "href": "assignments/lab-6.html#your-task",
    "title": "Assignment 06: Advanced Imports and Data Manipulations",
    "section": "ğŸ§  Your task",
    "text": "ğŸ§  Your task\nYou are a data analyst exploring taxi activity in New York. Your goal is to import, transform, and analyze one or more subsets of the NYC taxi data using advanced import and manipulation techniques.\nYou decide: - which category of taxi trips to analyze (Yellow, Green, FHV, etc.), - which time period to focus on (for example, one month, one quarter, or a comparison between months/years).\n\nâš™ï¸ Step 1 â€” Data import\nChoose an import method suited for working with large files:\n\nUse arrow::open_dataset() to read directly from a Parquet/CSV directory.\nOr use DuckDB to query data locally.\nYou can also use partitioned datasets (e.g.Â year/month folders) to analyze multiple files efficiently.\n\n\n\n\n\n\n\nTip\n\n\n\nYou can also save your selected subset in .csv format for performance comparison.\n\n\n\n\nğŸ”„ Step 2 â€” Data exploration and transformation\nPerform data manipulation using SQL queries with dplyr, arrow, or polars.\nPossible ideas:\n\nCompute average trip distance or fare month-to-month.\nCompare tips by vendor or taxi type (Yellow vs Green).\nIdentify peak hours or days with the highest number of trips.\nDetect outliers or anomalies in trip duration or amount.\n\n\n\n\n\n\n\nTip\n\n\n\nItâ€™s only a suggestion. Feel free to explore other interesting questions.\n\n\n\n\nğŸ“Š Step 3 â€” Visualization and insights\nCreate 1â€“2 visualizations that best communicate your findings, such as:\n\nLine chart: monthly average trip distance or total rides.\nBar chart: tip comparison across taxi types.\nScatter plot: temperature vs.Â number of rides.\n\nMake sure your plots have:\n\nclear titles, labels, and legends;\nan interpretation paragraph in your Quarto report.",
    "crumbs": [
      "ğŸ‹ï¸ Weekly assignments",
      "06 - Advanced Imports and Data Manipulations"
    ]
  },
  {
    "objectID": "assignments/lab-6.html#submission",
    "href": "assignments/lab-6.html#submission",
    "title": "Assignment 06: Advanced Imports and Data Manipulations",
    "section": "ğŸ“¤ Submission",
    "text": "ğŸ“¤ Submission\nSubmit a Quarto report (.qmd and PDF/HTML) that includes:\n\ndescription of your question and selected dataset subset;\ndata import process (DuckDB, Arrow, or Polars);\ncleaning, transformation, and merging steps;\nvisualizations with comments;\n(optional) performance note comparing two import methods.\n\nIf you use HTML, publish your report at Quarto Pub or GitHub Pages and include the link in your submission.",
    "crumbs": [
      "ğŸ‹ï¸ Weekly assignments",
      "06 - Advanced Imports and Data Manipulations"
    ]
  },
  {
    "objectID": "assignments/lab-4.html",
    "href": "assignments/lab-4.html",
    "title": "Assignment 04: Data Visualization â€” ggplot2 and Beyond",
    "section": "",
    "text": "By completing this assignment, you will learn to:\n\nCraft a data-driven narrative using ggplot2.\nCreate sophisticated multi-layered plots (e.g., combining geom_point, geom_smooth, geom_text etc.).\nMaster advanced aesthetic mappings to convey complex relationships.\nDesign custom themes and annotations to create publication-ready visualizations.\nExplore extensions to ggplot2 to create dynamic or composite plots.\nThink critically about visualization choices and justify them in a clear, concise manner.",
    "crumbs": [
      "ğŸ‹ï¸ Weekly assignments",
      "04 - Data Visualization with ggplot2"
    ]
  },
  {
    "objectID": "assignments/lab-4.html#learning-objectives",
    "href": "assignments/lab-4.html#learning-objectives",
    "title": "Assignment 04: Data Visualization â€” ggplot2 and Beyond",
    "section": "",
    "text": "By completing this assignment, you will learn to:\n\nCraft a data-driven narrative using ggplot2.\nCreate sophisticated multi-layered plots (e.g., combining geom_point, geom_smooth, geom_text etc.).\nMaster advanced aesthetic mappings to convey complex relationships.\nDesign custom themes and annotations to create publication-ready visualizations.\nExplore extensions to ggplot2 to create dynamic or composite plots.\nThink critically about visualization choices and justify them in a clear, concise manner.",
    "crumbs": [
      "ğŸ‹ï¸ Weekly assignments",
      "04 - Data Visualization with ggplot2"
    ]
  },
  {
    "objectID": "assignments/lab-4.html#dataset",
    "href": "assignments/lab-4.html#dataset",
    "title": "Assignment 04: Data Visualization â€” ggplot2 and Beyond",
    "section": "ğŸ“‚ Dataset",
    "text": "ğŸ“‚ Dataset\nReuse the dataset from Assignment 02 (the one you tidied and wrangled). You already prepared it â€” now itâ€™s time to visualize! Or choose a new dataset from TidyTuesday. Especially if itâ€™s a Netflix dataset ğŸ™ƒ",
    "crumbs": [
      "ğŸ‹ï¸ Weekly assignments",
      "04 - Data Visualization with ggplot2"
    ]
  },
  {
    "objectID": "assignments/lab-4.html#the-challenge",
    "href": "assignments/lab-4.html#the-challenge",
    "title": "Assignment 04: Data Visualization â€” ggplot2 and Beyond",
    "section": "ğŸ“ The Challenge",
    "text": "ğŸ“ The Challenge\nImagine you are a data journalist. Your task is to create a visual story using your dataset.\nYour audience: curious readers who donâ€™t know R, statistics, or data science.\nYour mission: make the data speak clearly, visually, and persuasively.\n\nInstructions\n\nChoose a dataset (your tidied dataset from Assignment 02 or a new one from TidyTuesday).\nIdentify a compelling story or insight within the data that you want to communicate.\nCreate a series of visualizations using ggplot2 (and its extensions if desired) to illustrate your story. Aim for at least 3-5 different plots.\nWrite a narrative that ties the visualizations together, explaining the insights and significance of each plot.\nUse Quarto to compile your visualizations and narrative into a cohesive document.\n\n\n\n\n\n\n\nTip\n\n\n\nHere are some resources to help you get started:\n\nThe R Graph Gallery â€” a collection of hundreds of charts made with R.\nggplot2 extensions â€” explore various extensions to ggplot2 for enhanced functionality.\nMetBrewer package for color palettes inspired by works at the Metropolitan Museum of Art: GitHub.\nSlides from seminar (Even More) Exciting Data Visualizations with ggplot2 Extensions by CÃ©dric Scherer",
    "crumbs": [
      "ğŸ‹ï¸ Weekly assignments",
      "04 - Data Visualization with ggplot2"
    ]
  },
  {
    "objectID": "assignments/lab-4.html#submission",
    "href": "assignments/lab-4.html#submission",
    "title": "Assignment 04: Data Visualization â€” ggplot2 and Beyond",
    "section": "ğŸ“¤ Submission",
    "text": "ğŸ“¤ Submission\nSubmit the .qmd file, and rendered document (PDF) to the assignment submission portal.",
    "crumbs": [
      "ğŸ‹ï¸ Weekly assignments",
      "04 - Data Visualization with ggplot2"
    ]
  },
  {
    "objectID": "assignments/lab-2.html",
    "href": "assignments/lab-2.html",
    "title": "Assignment 02: Data Wrangling & Tidying with Tidyverse",
    "section": "",
    "text": "After completing this assignment, you should be able to:\n\nImport and explore real-world datasets from the TidyTuesday project.\nUse dplyr functions to filter, arrange, and select subsets of data.\nCreate new variables with mutate().\nSummarize grouped data with group_by() + summarise().\nReshape messy datasets using pivot_longer() and pivot_wider().\nApply piping (|&gt;) for readable workflows.",
    "crumbs": [
      "ğŸ‹ï¸ Weekly assignments",
      "02 - Data Wrangling & Tidying with Tidyverse"
    ]
  },
  {
    "objectID": "assignments/lab-2.html#learning-objectives",
    "href": "assignments/lab-2.html#learning-objectives",
    "title": "Assignment 02: Data Wrangling & Tidying with Tidyverse",
    "section": "",
    "text": "After completing this assignment, you should be able to:\n\nImport and explore real-world datasets from the TidyTuesday project.\nUse dplyr functions to filter, arrange, and select subsets of data.\nCreate new variables with mutate().\nSummarize grouped data with group_by() + summarise().\nReshape messy datasets using pivot_longer() and pivot_wider().\nApply piping (|&gt;) for readable workflows.",
    "crumbs": [
      "ğŸ‹ï¸ Weekly assignments",
      "02 - Data Wrangling & Tidying with Tidyverse"
    ]
  },
  {
    "objectID": "assignments/lab-2.html#dataset",
    "href": "assignments/lab-2.html#dataset",
    "title": "Assignment 02: Data Wrangling & Tidying with Tidyverse",
    "section": "ğŸ“‚ Dataset",
    "text": "ğŸ“‚ Dataset\nFor this assignment, each student should choose any dataset from the TidyTuesday GitHub repository.\n\n\n\n\n\n\nTipğŸ‘‰ Recommendations\n\n\n\n\nPick a dataset that interests you (e.g., food, sports, education, social trends, space, animals).\nAvoid datasets that are too small (less than 100 rows) or too complicated (multiple nested tables).\nRead the description and context of the dataset on the TidyTuesday page to understand its variables and structure.\nFrom my experience, I know it is very tempting to find â€œthe oneâ€ dataset. Do not spend too much time searching. Just pick one and start working with it.\n\n\n\nLoad the dataset directly from GitHub using readr::read_csv() or read the instructions on the TidyTuesday page for that week. For example, you can use the following code to load the dataset for the week of October 22, 2024:\n\nlibrary(tidytuesdayR)\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-10-22')\n\n---- Compiling #TidyTuesday Information for 2024-10-22 ----\n--- There is 1 file available ---\n\n\nâ”€â”€ Downloading files â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  1 of 1: \"cia_factbook.csv\"\n\ntuesdata$cia_factbook\n\n# A tibble: 259 Ã— 11\n   country       area birth_rate death_rate infant_mortality_rate internet_users\n   &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;                 &lt;dbl&gt;          &lt;dbl&gt;\n 1 Russia      1.71e7       11.9      13.8                   7.08       40853000\n 2 Canada      9.98e6       10.3       8.31                  4.71       26960000\n 3 United Staâ€¦ 9.83e6       13.4       8.15                  6.17      245000000\n 4 China       9.60e6       12.2       7.44                 14.8       389000000\n 5 Brazil      8.51e6       14.7       6.54                 19.2        75982000\n 6 Australia   7.74e6       12.2       7.07                  4.43       15810000\n 7 India       3.29e6       19.9       7.35                 43.2        61338000\n 8 Argentina   2.78e6       16.9       7.34                  9.96       13694000\n 9 Kazakhstan  2.72e6       19.6       8.31                 21.6         5299000\n10 Algeria     2.38e6       24.0       4.31                 21.8              NA\n# â„¹ 249 more rows\n# â„¹ 5 more variables: life_exp_at_birth &lt;dbl&gt;, maternal_mortality_rate &lt;dbl&gt;,\n#   net_migration_rate &lt;dbl&gt;, population &lt;dbl&gt;, population_growth_rate &lt;dbl&gt;",
    "crumbs": [
      "ğŸ‹ï¸ Weekly assignments",
      "02 - Data Wrangling & Tidying with Tidyverse"
    ]
  },
  {
    "objectID": "assignments/lab-2.html#tasks",
    "href": "assignments/lab-2.html#tasks",
    "title": "Assignment 02: Data Wrangling & Tidying with Tidyverse",
    "section": "ğŸ“ Tasks",
    "text": "ğŸ“ Tasks\n\n\n\n\n\n\nImportant\n\n\n\nBelow is only a basic scenario. Please take the initiative of a researcher â€” ask questions about the data and find answers to them. This is much more interesting than simply â€œmemorizingâ€ the languageâ€™s syntax.\n\n\n\nImport and Explore the Data:\n\nLoad the dataset into R.\nUse functions like head(), glimpse(), and summary() to understand its structure and contents.\n\nFiltering & Selecting:\n\nApply at least two filtering conditions (e.g., values above a threshold, excluding missing data).\nSelect 3â€“5 relevant variables. Show the first 10 rows.\nFind the top 5 records according to some numeric variable of your choice.\n\nMutating Variables:\n\nCreate a new column that is a ratio, difference, or transformation of existing variables.\nIdentify the record with the maximum value of this new variable.\n\nGrouping & Summarizing:\n\nGroup the data by a categorical variable and calculate summary statistics (mean, median, count) for a numeric variable.\nPresent the results in a clear table.\nInterpret the findings.\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor some datasets, you may need to join multiple tables or tidy the data first using pivot_longer() or pivot_wider(). Feel free to do so if necessary.",
    "crumbs": [
      "ğŸ‹ï¸ Weekly assignments",
      "02 - Data Wrangling & Tidying with Tidyverse"
    ]
  },
  {
    "objectID": "assignments/lab-2.html#submission",
    "href": "assignments/lab-2.html#submission",
    "title": "Assignment 02: Data Wrangling & Tidying with Tidyverse",
    "section": "ğŸ“¤ Submission",
    "text": "ğŸ“¤ Submission\n\nSubmit your .r file.\nYour code must run without errors.\nAdd short comments explaining what each step does.",
    "crumbs": [
      "ğŸ‹ï¸ Weekly assignments",
      "02 - Data Wrangling & Tidying with Tidyverse"
    ]
  },
  {
    "objectID": "assignments/lab-1.html",
    "href": "assignments/lab-1.html",
    "title": "Assignment 01: Welcome to R",
    "section": "",
    "text": "Letâ€™s look how you can read formulas in math notation and translate them into R code.\nYour first task is to compute the Pearson linear correlation coefficient1 using its mathematical formula:\n\\[\nr = \\frac{\\sum_{i=1}^n \\left(x_i - \\frac{1}{n}\\sum_{j=1}^n x_j\\right)\\left(y_i - \\frac{1}{n}\\sum_{j=1}^n y_j\\right)}\n         {\\sqrt{\\sum_{i=1}^n \\left(x_i - \\frac{1}{n}\\sum_{j=1}^n x_j\\right)^2} \\sqrt{\\sum_{i=1}^n \\left(y_i - \\frac{1}{n}\\sum_{j=1}^n y_j\\right)^2}}\n\\]\n\n\n\n\n\n\nNoteExplanation of Pearson correlation\n\n\n\n\n\n\n\n\n\nLet x and y be two vectors of identical lengths n, say. Next code chunk generates two such vectors of length n = 100:\n\nn &lt;- 100\nx &lt;- runif(n, 0, 10)\ny &lt;- 2 * x + runif(n, -1, 1)\n\n\n\n\n\n\n\nWarning\n\n\n\nDo not use the built-in cor() function.\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nIn this exercise, you need to calculate the correlation coefficient manually using the formula above. And please, do not use loops, as they are not necessary here.\nFunctions that may be useful: mean(), length(), sqrt().\nRead the documentation for the runif(). Why does it return different values each time you run it? How can you make it return the same values each time?",
    "crumbs": [
      "ğŸ‹ï¸ Weekly assignments",
      "01 - Welcome to R"
    ]
  },
  {
    "objectID": "assignments/lab-1.html#footnotes",
    "href": "assignments/lab-1.html#footnotes",
    "title": "Assignment 01: Welcome to R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou will use this formula in couple of courses, so it is worth memorizing it.â†©ï¸\nThe dataset is adapted from the NASA Exoplanet Archive. It contains information about various exoplanets discovered outside our solar system.â†©ï¸",
    "crumbs": [
      "ğŸ‹ï¸ Weekly assignments",
      "01 - Welcome to R"
    ]
  },
  {
    "objectID": "assignments/lab-3.html",
    "href": "assignments/lab-3.html",
    "title": "Assignment 03: Reproducible publishing with Quarto",
    "section": "",
    "text": "After completing this assignment, you should be able to:\n\nCreate reproducible documents using Quarto.\nIntegrate R code and narrative text seamlessly.\nRender documents to various formats (HTML, PDF, Word).\nUse Quarto extensions to enhance document functionality and appearance.",
    "crumbs": [
      "ğŸ‹ï¸ Weekly assignments",
      "03 - Reproducible publishing with Quarto"
    ]
  },
  {
    "objectID": "assignments/lab-3.html#learning-objectives",
    "href": "assignments/lab-3.html#learning-objectives",
    "title": "Assignment 03: Reproducible publishing with Quarto",
    "section": "",
    "text": "After completing this assignment, you should be able to:\n\nCreate reproducible documents using Quarto.\nIntegrate R code and narrative text seamlessly.\nRender documents to various formats (HTML, PDF, Word).\nUse Quarto extensions to enhance document functionality and appearance.",
    "crumbs": [
      "ğŸ‹ï¸ Weekly assignments",
      "03 - Reproducible publishing with Quarto"
    ]
  },
  {
    "objectID": "assignments/lab-3.html#tasks-your-cv",
    "href": "assignments/lab-3.html#tasks-your-cv",
    "title": "Assignment 03: Reproducible publishing with Quarto",
    "section": "ğŸ“ Tasks: your CV",
    "text": "ğŸ“ Tasks: your CV\nIn this assignment, you will create a simple CV using Quarto. The CV should include the following sections1:\n\nPersonal Information (name, contact details, LinkedIn/GitHub links)\nEducation (degrees, institutions, graduation dates)\nWork Experience (job titles, companies, dates, responsibilities)\nSkills (programming languages, tools, technologies)\nProjects (brief descriptions and links to GitHub repositories or live demos). If you do not have any projects yet, you can indicate which courses you have completed and what skills you have acquired.\n\n\n\n\n\n\n\nNote\n\n\n\nTry to think ahead: as you gain new skills and experience, you must update your resume. It is a good idea to keep all your achievements in a separate file and refer to it to update/add to your achievements. Use code chunks for this.\n\n\n\nInstructions\n\nCreate a new Quarto document (.qmd file).\nCreate a CV using Quarto.\nAdd a YAML header at the top of the document with appropriate metadata (title, author, date, output format).\nRender the Quarto document to PDF format.\n\n\n\n\n\n\n\nTip\n\n\n\nNext tips can help you to make your CV more attractive and informative:\n\nYou can use the Quarto CV template as a starting point.\nIconify, Font Awesome and Academicons extensions for Quarto provide a wide range of icons that you can use to enhance the visual appeal of your CV.\nYou can use gt package and/or kableExtra package to create beautiful tables in your CV.\nQuarto Publishing provides detailed instructions on how to publish your Quarto documents to various platforms.",
    "crumbs": [
      "ğŸ‹ï¸ Weekly assignments",
      "03 - Reproducible publishing with Quarto"
    ]
  },
  {
    "objectID": "assignments/lab-3.html#submission",
    "href": "assignments/lab-3.html#submission",
    "title": "Assignment 03: Reproducible publishing with Quarto",
    "section": "ğŸ“¤ Submission",
    "text": "ğŸ“¤ Submission\nSubmit the .qmd file, and rendered document (PDF) to the assignment submission portal.",
    "crumbs": [
      "ğŸ‹ï¸ Weekly assignments",
      "03 - Reproducible publishing with Quarto"
    ]
  },
  {
    "objectID": "assignments/lab-3.html#footnotes",
    "href": "assignments/lab-3.html#footnotes",
    "title": "Assignment 03: Reproducible publishing with Quarto",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nItâ€™s only a suggestion. You can modify the sections as per your preference.â†©ï¸",
    "crumbs": [
      "ğŸ‹ï¸ Weekly assignments",
      "03 - Reproducible publishing with Quarto"
    ]
  },
  {
    "objectID": "assignments/lab-5.html",
    "href": "assignments/lab-5.html",
    "title": "Assignment 05: Web scraping and APIs",
    "section": "",
    "text": "By the end of this assignment, you will be able to:\n\nScrape data from a webpage using the rvest package.\nRetrieve data from a public API using the httr2 and jsonlite packages.\nClean and structure the collected data into a tidy format.\nVisualize the data to explore and communicate insights.\nDocument your data collection and cleaning process.",
    "crumbs": [
      "ğŸ‹ï¸ Weekly assignments",
      "05 - Web Scraping and APIs"
    ]
  },
  {
    "objectID": "assignments/lab-5.html#learning-objectives",
    "href": "assignments/lab-5.html#learning-objectives",
    "title": "Assignment 05: Web scraping and APIs",
    "section": "",
    "text": "By the end of this assignment, you will be able to:\n\nScrape data from a webpage using the rvest package.\nRetrieve data from a public API using the httr2 and jsonlite packages.\nClean and structure the collected data into a tidy format.\nVisualize the data to explore and communicate insights.\nDocument your data collection and cleaning process.",
    "crumbs": [
      "ğŸ‹ï¸ Weekly assignments",
      "05 - Web Scraping and APIs"
    ]
  },
  {
    "objectID": "assignments/lab-5.html#task-1.-basic",
    "href": "assignments/lab-5.html#task-1.-basic",
    "title": "Assignment 05: Web scraping and APIs",
    "section": "ğŸ“ Task 1. Basic",
    "text": "ğŸ“ Task 1. Basic\nIn this task, you will work with a website where data is directly embedded in the HTML page. Your goal is to choose a website of interest and collect structured data from it.\n\nInstructions\n\nChoose a source. Find a webpage that contains data in a table or a list that can be converted into a table. This can be anything that interests you.\n\nIdeas for inspiration:\n\nSports competition results (e.g., a football league table, Olympic Games results).\nA list of movies, books, or music albums with ratings (e.g., IMDb Top 250 movies, bestseller lists).\nData from Wikipedia (e.g., a list of the worldâ€™s tallest buildings, demographic data for countries).\nA product catalog from a small online store (avoid large marketplaces that actively protect against scraping).\n\n\nScrape the data using rvest. Use SelectorGadget or your browserâ€™s developer tools to find the correct CSS selectors for the elements you need.\n\nUse functions like read_html(), html_elements(), html_table() or html_text2() to extract the data.\n\nClean and structure the data. Transform the raw data into a tidy tibble.\n\nUse janitor::clean_names() to standardize column names.\nUse dplyr and stringr to clean the data: remove unnecessary characters, and convert data types (e.g., from text to numbers or dates).\n\nSave the cleaned data.\nMake some visualizations to explore the data.\nDocument your process. Write a brief summary of the steps you took, any challenges you faced, and how you overcame them.",
    "crumbs": [
      "ğŸ‹ï¸ Weekly assignments",
      "05 - Web Scraping and APIs"
    ]
  },
  {
    "objectID": "assignments/lab-5.html#task-2.-advanced",
    "href": "assignments/lab-5.html#task-2.-advanced",
    "title": "Assignment 05: Web scraping and APIs",
    "section": "ğŸ“ Task 2. Advanced",
    "text": "ğŸ“ Task 2. Advanced\nThis task will teach you how to retrieve data from web services via their Application Programming Interfaces (APIs). This is a more reliable and â€œpoliteâ€ way to collect data than HTML scraping.\n\nInstructions\n\nFind a public API. There are countless services that provide data through an API. This usually requires a free registration to obtain an API key.\n\nIdeas for inspiration:\n\nOpenWeatherMap â€” Weather data.\nThe Movie Database (TMDB) â€” Information about movies, actors, etc.\nFRED â€” Economic data (as seen in the lecture).\nNASA APIs â€” Data and photos from NASA.\n\n\nRegister and get an API key. Store it securely, for example, using usethis::edit_r_environ().\nMake a request. Using httr2 and jsonlite, send a request to the API, receive the response in JSON format, and parse it.\nCreate a dataframe. Extract the necessary data from the resulting structure and convert it into a tidy table.\nSave the cleaned data.\nMake some visualizations to explore the data.\nDocument your process. Write a brief summary of the steps you took, any challenges you faced, and how you overcame them.",
    "crumbs": [
      "ğŸ‹ï¸ Weekly assignments",
      "05 - Web Scraping and APIs"
    ]
  },
  {
    "objectID": "assignments/lab-5.html#submission",
    "href": "assignments/lab-5.html#submission",
    "title": "Assignment 05: Web scraping and APIs",
    "section": "ğŸ“¤ Submission",
    "text": "ğŸ“¤ Submission\nSubmit the .qmd file, and rendered document (PDF) to the assignment submission portal.",
    "crumbs": [
      "ğŸ‹ï¸ Weekly assignments",
      "05 - Web Scraping and APIs"
    ]
  },
  {
    "objectID": "assignments/lab-7.html",
    "href": "assignments/lab-7.html",
    "title": "Assignment 07: Spatial analysis in R",
    "section": "",
    "text": "Tip\n\n\n\nI highly recommend checking out the channel â–¶ï¸ Milos Makes Maps on YouTube and his ğŸ˜º GitHub for tutorials on spatial data analysis and mapping in R!",
    "crumbs": [
      "ğŸ‹ï¸ Weekly assignments",
      "07 - Spatial analysis in R"
    ]
  },
  {
    "objectID": "assignments/lab-7.html#learning-objectives",
    "href": "assignments/lab-7.html#learning-objectives",
    "title": "Assignment 07: Spatial analysis in R",
    "section": "ğŸ¯ Learning Objectives",
    "text": "ğŸ¯ Learning Objectives\nBy the end of this assignment, you will be able to:\n\nUnderstand and work with spatial data in R using the sf package.\nRead and manipulate shapefiles.\nCreate choropleth maps to visualize spatial patterns.\nCombine tabular data with spatial data for thematic mapping.\nCustomize map aesthetics, including projections and color schemes.\nDocument your spatial analysis process in a Quarto report.",
    "crumbs": [
      "ğŸ‹ï¸ Weekly assignments",
      "07 - Spatial analysis in R"
    ]
  },
  {
    "objectID": "assignments/lab-7.html#datasets",
    "href": "assignments/lab-7.html#datasets",
    "title": "Assignment 07: Spatial analysis in R",
    "section": "ğŸ“‚ Datasets",
    "text": "ğŸ“‚ Datasets\nFor this assignment, you will need two types of data:\n\nSpatial Data: A shapefile with the administrative boundaries of Ukraineâ€™s oblasts.\nTabular Data: A dataset containing some variable to visualize across the oblasts.\n\n\n\n\n\n\n\nNote\n\n\n\nYou can use any other country or region if you prefer, but make sure to adjust the instructions accordingly.\n\n\nFor the tabular data, you have three options. Choose one:\n\nOption A: Air Raid Sirens Data: Use the dataset on air raid sirens in Ukraine from this source. You will need to aggregate the data by oblast.\nOption B: State Statistics Service of Ukraine: Find a dataset of your choice on the State Statistics Service of Ukraine.\nOption C: Your Own Data: Find or create your own dataset with data for each of Ukraineâ€™s oblasts.",
    "crumbs": [
      "ğŸ‹ï¸ Weekly assignments",
      "07 - Spatial analysis in R"
    ]
  },
  {
    "objectID": "assignments/lab-7.html#tasks",
    "href": "assignments/lab-7.html#tasks",
    "title": "Assignment 07: Spatial analysis in R",
    "section": "ğŸ“ Tasks",
    "text": "ğŸ“ Tasks\n\nâš™ï¸ Step 1: Set Up Your Project\n\nCreate a new Quarto document (.qmd) for your assignment.\nLoad the necessary libraries: sf, tidyverse, and any others you might need.\nDownload the shapefile of Ukraineâ€™s oblasts and place it in a data subdirectory within your project.\n\n\n\nğŸ”„ Step 2: Load and Prepare Data\n\nLoad the Spatial Data: Read the shapefile into R using sf::read_sf().\nLoad the Tabular Data: Load the tabular dataset you chose.\nData Cleaning and Preparation:\n\nClean both datasets as needed. Pay special attention to the names of the oblasts to ensure they are consistent between the two datasets for a successful join.\nIf you chose the air raid sirens data, you will need to aggregate it to the oblast level (e.g., total number of alerts, total duration).\n\nJoin the Datasets: Perform a join (e.g., left_join()) to merge your tabular data with the spatial data based on the oblast names.\n\n\n\nğŸ“Š Step 3: Create a Choropleth Map\n\nCreate a Basic Map: Use ggplot2 and geom_sf() to create a basic choropleth map of Ukraine, where the fill color of each oblast represents the value of the variable you are analyzing.\nCustomize the Map:\n\nAdd a title, subtitle, and caption.\nCustomize the color scale using scale_fill_*().\nExperiment with different map projections using coord_sf().\nAdjust the theme for a cleaner look (theme_void() is a good starting point).\n\nInterpret Your Map: Write a brief interpretation of the patterns you observe in your map. What does the map reveal about your chosen variable?",
    "crumbs": [
      "ğŸ‹ï¸ Weekly assignments",
      "07 - Spatial analysis in R"
    ]
  },
  {
    "objectID": "assignments/lab-7.html#bonus-challenge-optional",
    "href": "assignments/lab-7.html#bonus-challenge-optional",
    "title": "Assignment 07: Spatial analysis in R",
    "section": "ğŸ’¡ Bonus Challenge (Optional)",
    "text": "ğŸ’¡ Bonus Challenge (Optional)\n\nCreate an additional map that visualizes a different variable or uses a different mapping technique (e.g., a point map, a line map).\nAdd labels to the oblasts on your map.\nFind and add major cities as points on your map.",
    "crumbs": [
      "ğŸ‹ï¸ Weekly assignments",
      "07 - Spatial analysis in R"
    ]
  },
  {
    "objectID": "assignments/lab-7.html#submission",
    "href": "assignments/lab-7.html#submission",
    "title": "Assignment 07: Spatial analysis in R",
    "section": "ğŸ“¤ Submission",
    "text": "ğŸ“¤ Submission\nSubmit a Quarto report (.qmd and the rendered PDF or HTML document) that includes:\n\nA description of your chosen dataset.\nAll the code for loading, cleaning, and joining the data.\nThe final choropleth map.\nYour interpretation of the map.\n\nIf you render to HTML, please publish your report on Quarto Pub or GitHub Pages and include the link in your submission.",
    "crumbs": [
      "ğŸ‹ï¸ Weekly assignments",
      "07 - Spatial analysis in R"
    ]
  },
  {
    "objectID": "course-links.html",
    "href": "course-links.html",
    "title": "Useful links",
    "section": "",
    "text": "Textbooks\nğŸ”— R for Data Science, 2nd Edition\nğŸ”— ggplot2: Elegant Graphics for Data Analysis, 3rd Edition\nğŸ”— Advanced R\nğŸ”— Spatial Data Science\n\n\nPackage documentation\nğŸ”— tidyverse: tidyverse.org",
    "crumbs": [
      "ğŸ‘‹Course information",
      "Useful links"
    ]
  },
  {
    "objectID": "course-team.html",
    "href": "course-team.html",
    "title": "Instructor",
    "section": "",
    "text": "Dr.Â Ihor Miroshnychenko\nğŸ‘‹ Course Faculty\nIhor holds a PhD in Economics and brings a wealth of academic and practical experience to the course. Currently, he is a Teacher at the  Computer Science Department at Kyiv School of Economics.\nHis research and professional focus lie in Data Science, Statistics, and Machine Learning. He is passionate about equipping students with the tools to analyze data effectively and communicate findings clearly.\nBeyond the Data:\nWhen not teaching or coding, Ihor is a true explorer of both real and imaginary worlds.\n\nğŸ¸ Musician: Guitar player and a dedicated Metalhead.\nâš”ï¸ History & Fantasy: Engaged in historical reconstruction and is a big fan of the Middle-earth universe.\nğŸ“· Hobbies: Photography, gaming, reading, and traveling.\n\n\n\n\nğŸ“¬ Contact & Connect\nFind me on the web or send a signal:\n\n Email: imiroshnychenko@kse.org.ua\n Web: aranaur.rbind.io\n GitHub: @aranaur\n Telegram @araprof\n LinkedIn: ihormiroshnychenko\n\n\n\n\nğŸ§ Co-pilot\n\nPipe the Penguin\n\nRole: Teaching Assistant (STAT150 Edition)\nExpertise: Sliding on ice, eating fish, and piping data (|&gt;)\nOffice Hours: Whenever there is fish available.\n\nPipe assists Dr.Â Ihor in preparing materials and ensures that all datasets are strictly â€œpenguin-approvedâ€.\n\n\n\n\n\n\nNoteTeaching Philosophy\n\n\n\nâ€œI believe that data science is a powerful tool for understanding the world around us. My goal is to help students develop a deep appreciation for the role of data in decision-making and to equip them with the skills to analyze data effectively and communicate their findings clearly.â€",
    "crumbs": [
      "ğŸ‘‹Course information",
      "Teaching team"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Course overview",
    "section": "",
    "text": "Welcome to the homepage for STAT150: R for Data Science. This course is taught by Dr.Â Ihor Miroshnychenko in Autumn 2025 at the Kyiv School of Economics.",
    "crumbs": [
      "ğŸ‘‹Course information",
      "Overview"
    ]
  },
  {
    "objectID": "index.html#why-learn-r",
    "href": "index.html#why-learn-r",
    "title": "Course overview",
    "section": "ğŸš€ Why learn R?",
    "text": "ğŸš€ Why learn R?\nData is the new language of business, economics, and science. In this course, you wonâ€™t just learn â€œhow to codeâ€ â€” you will learn how to discover stories hidden within data.\nR is not just a programming language; it is a vast ecosystem designed specifically for data analysis. Whether you want to visualize economic trends, analyze marketing metrics, or automate reports, R is the industry standard tool used by companies like Google, Facebook, and Uber.\n\n\n\n\n\n\nNoteIs this course for me?\n\n\n\nYes! This course is designed for beginners. We assume no prior programming experience. We start from zero and build up to professional-grade data analysis skills using the modern â€œTidyverseâ€ approach.",
    "crumbs": [
      "ğŸ‘‹Course information",
      "Overview"
    ]
  },
  {
    "objectID": "index.html#what-you-will-learn",
    "href": "index.html#what-you-will-learn",
    "title": "Course overview",
    "section": "ğŸ¯ What you will learn",
    "text": "ğŸ¯ What you will learn\nBy the end of this course, you will possess the toolkit to turn raw data into actionable insights. We will focus on the practical cycle of Data Science:\n\nImport: Getting data from files, databases, or APIs.\nTidy: Cleaning and organizing messy real-world data.\nTransform: Selecting, filtering, and summarizing key information.\nVisualize: Creating stunning, publication-quality graphics with ggplot2.\nCommunicate: Publishing your results using Quarto and Markdown.",
    "crumbs": [
      "ğŸ‘‹Course information",
      "Overview"
    ]
  },
  {
    "objectID": "index.html#course-philosophy",
    "href": "index.html#course-philosophy",
    "title": "Course overview",
    "section": "ğŸ› ï¸ Course Philosophy",
    "text": "ğŸ› ï¸ Course Philosophy\nWe believe in hands-on learning. You wonâ€™t just memorize syntax; you will solve real problems. The curriculum follows the philosophy of the famous book R for Data Science by Hadley Wickham, focusing on writing code that is easy to read, easy to share, and easy to reproduce.\n\nâ€œData science is an exciting discipline that allows you to turn raw data into understanding, insight, and knowledge.â€ â€” Hadley Wickham",
    "crumbs": [
      "ğŸ‘‹Course information",
      "Overview"
    ]
  },
  {
    "objectID": "index.html#logistics",
    "href": "index.html#logistics",
    "title": "Course overview",
    "section": "ğŸ“… Logistics",
    "text": "ğŸ“… Logistics\n\nInstructor: Dr.Â Ihor Miroshnychenko\nInstitution: Kyiv School of Economics (KSE)\nTerm: Autumn 2025\nFormat: Offline\n\nReady to start your data journey? Navigate to the Syllabus tab to begin!",
    "crumbs": [
      "ğŸ‘‹Course information",
      "Overview"
    ]
  },
  {
    "objectID": "index.html#course-identity",
    "href": "index.html#course-identity",
    "title": "Course overview",
    "section": "ğŸ¨ Course Identity",
    "text": "ğŸ¨ Course Identity\nIn the spirit of reproducible research, the visual identity for this course is created entirely using code! You can explore the collection of generated KSE course stickers in this GitHub repository.\nCurious how the logo above was created? Click to reveal the source code.",
    "crumbs": [
      "ğŸ‘‹Course information",
      "Overview"
    ]
  },
  {
    "objectID": "lectures/lecture-1.html",
    "href": "lectures/lecture-1.html",
    "title": "Welcome to R",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "ğŸ“œ Slides",
      "01 - Introduction to R and RStudio"
    ]
  },
  {
    "objectID": "lectures/lecture-1.html#slides",
    "href": "lectures/lecture-1.html#slides",
    "title": "Welcome to R",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "ğŸ“œ Slides",
      "01 - Introduction to R and RStudio"
    ]
  },
  {
    "objectID": "lectures/lecture-3.html",
    "href": "lectures/lecture-3.html",
    "title": "Quarto - reproducible publishing",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "ğŸ“œ Slides",
      "03 - Reproducible Research with Quarto"
    ]
  },
  {
    "objectID": "lectures/lecture-3.html#slides",
    "href": "lectures/lecture-3.html#slides",
    "title": "Quarto - reproducible publishing",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "ğŸ“œ Slides",
      "03 - Reproducible Research with Quarto"
    ]
  },
  {
    "objectID": "lectures/lecture-5.html",
    "href": "lectures/lecture-5.html",
    "title": "Webscraping",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "ğŸ“œ Slides",
      "05 - Web Scraping and APIs"
    ]
  },
  {
    "objectID": "lectures/lecture-5.html#slides",
    "href": "lectures/lecture-5.html#slides",
    "title": "Webscraping",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "ğŸ“œ Slides",
      "05 - Web Scraping and APIs"
    ]
  },
  {
    "objectID": "lectures/lecture-7.html",
    "href": "lectures/lecture-7.html",
    "title": "Spatial analysis in R",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "ğŸ“œ Slides",
      "07 - Spatial Data Analysis in R"
    ]
  },
  {
    "objectID": "lectures/lecture-7.html#slides",
    "href": "lectures/lecture-7.html#slides",
    "title": "Spatial analysis in R",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "ğŸ“œ Slides",
      "07 - Spatial Data Analysis in R"
    ]
  }
]